{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e81ccc-bcc7-4644-911f-4f6f4fef9b32",
   "metadata": {},
   "source": [
    "<h1>Federated Learning: Scheduler Comparison</h1>\n",
    "\n",
    "<t>This notebook compares *Random*, *Age-based*, and *Age-Of-Update OR DataShapley* (AoU), and Version Age-based Schedulers using the MNIST and CIFAR-10 Datasets.\n",
    "\n",
    "> Two models are considered,\n",
    "> 1. A Multi-layer Perceptron comprising of two hidden layers with 64 units each, utilizing the ReLU activation function\n",
    "> 2. A Convolutional Neural Network encompassing two convolutional layers with max pooling, two fully connected layers, and a softmax output layer\n",
    "\n",
    "> Two types of data distributions were considered when using the MNIST Dataset,\n",
    ">1. IID: Data is shuffled and then divided up across 100 clients each receiving 600 examples\n",
    ">2. Non-IID: Data is sorted by digit label, divided up into 200 'shards' of 300 examples, and then each client receieves 2 'shards\n",
    "\n",
    "</t>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41378390-df78-4bf9-a76b-43749e984635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Global Dependencies\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import Helper Libaries\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "filepath = \"C:/Users/aidan_000/Desktop/UNCC/Github/Fed-Learning/data\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb2ba65",
   "metadata": {},
   "source": [
    "<h2>Multi-layer Perceptron Model Architecture</h2>\n",
    "\n",
    "<t>*\"The initial experiment involves training an MLP with the MNIST dataset. This MLP comprises two\n",
    "hidden layers with 64 units each, utilizing the ReLU activation function\"*</t>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a326322",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)  # MNIST images are 28x28\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)  # 10 classes for MNIST\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bd4e94-9672-4540-99fa-b6362ed1fabe",
   "metadata": {},
   "source": [
    "<h2>CIFAR-10 CNN Model Architecture</h2>\n",
    "\n",
    "<t>*The second experiment focuses on training a Convolutional Neural Network CNN on the CIFAR-10 dataset. The CNN\n",
    "architecture encompasses two convolutional layers with max pooling, two fully connected layers, and a softmax output\n",
    "layer*</t>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42140649-bfe2-4b09-94b4-af2ed5f85f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
    "        self.fc1 = nn.Linear(64*8*8, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ae0be2-ae10-4ca1-bca7-15001291d51c",
   "metadata": {},
   "source": [
    "<h2>Federated Learning Algorithms</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d5b39-4b0d-4dd1-b328-b6e3de8a6e11",
   "metadata": {},
   "source": [
    "<h3>Global Aggregator</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6f9b57-59f8-4e51-b6f0-0764e8b14452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_aggregate(global_model, client_models):\n",
    "    global_dict = global_model.state_dict()\n",
    "    for k in global_dict.keys():\n",
    "        global_dict[k] = torch.stack([client_models[i].state_dict()[k] for i in range(len(client_models))], 0).mean(0)\n",
    "    global_model.load_state_dict(global_dict)\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(global_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580b49b5-990b-4555-8bb8-869eb940c037",
   "metadata": {},
   "source": [
    "<h3>Global Model Evaluation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab18ab62-95b1-4c67-8898-63c2d9eaf996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(global_model, validation_loader):\n",
    "    global_model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validation_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            output = global_model(inputs)\n",
    "            loss += nn.CrossEntropyLoss()(output, labels).item()  # Using the criterion\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    loss /= len(validation_loader.dataset)\n",
    "    accuracy = correct / len(validation_loader.dataset)\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae079702",
   "metadata": {},
   "source": [
    "<h3>Client Update</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c95129",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_update(client, optimizer, training_loader, epochs):\n",
    "    client.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (inputs, labels) in enumerate(training_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = client(inputs)\n",
    "            loss = F.cross_entropy(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cb069d-0da7-4b70-8677-2f91acec115a",
   "metadata": {},
   "source": [
    "<h2>Training with different schedulers</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e0745d-63e2-4913-b616-09d8a32910df",
   "metadata": {},
   "source": [
    "<h3>Training using Random Scheduling</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a4fdfc-ded9-40db-b8be-57f8941cc696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNG_training(model_type, global_model, lr, total_clients, clients_per_round, total_rounds, local_epochs, training_loaders, validation_loader):\n",
    "    clients = [model_type().to(device) for _ in range(total_clients)]\n",
    "\n",
    "    for models in clients:\n",
    "        models.load_state_dict(global_model.state_dict())\n",
    "    \n",
    "    opt = [optim.SGD(models.parameters(), lr=lr) for models in clients]\n",
    "\n",
    "    average_losses, valid_losses, valid_accuracies = [], [], []\n",
    "\n",
    "    global_start_time = time.time()\n",
    "    for round in range(total_rounds):\n",
    "        start_time = time.time()\n",
    "        clients_idx = np.random.permutation(total_clients)[:clients_per_round]\n",
    "\n",
    "        client_losses = 0\n",
    "        selected_models = []\n",
    "        \n",
    "        for i in range(clients_per_round):\n",
    "            clients[clients_idx[i]].load_state_dict(global_model.state_dict())\n",
    "            client_losses += client_update(clients[clients_idx[i]], opt[clients_idx[i]], training_loaders[clients_idx[i]], local_epochs)\n",
    "            selected_models.append(clients[clients_idx[i]])\n",
    "        \n",
    "        global_aggregate(global_model, selected_models)\n",
    "    \n",
    "        avg_loss = client_losses / clients_per_round\n",
    "        valid_loss , valid_accuracy = model_evaluation(global_model, validation_loader)\n",
    "\n",
    "        average_losses.append(avg_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accuracies.append(valid_accuracy)\n",
    "    \n",
    "        end_time = time.time()\n",
    "        round_time = end_time - start_time\n",
    "    \n",
    "        if (round % 10) == 0:\n",
    "            rounds_end_time = time.time()\n",
    "            rounds_time = rounds_end_time - global_start_time\n",
    "            print('Round {:3d}, Time (secs) {:.2f}: Average loss {:.4f}, Validation Loss {:.4f}, Validation Accuracy {:.4f}'.format(round + 1, rounds_time, avg_loss, valid_loss, valid_accuracy))\n",
    "    return average_losses, valid_losses, valid_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932137cf-fa6d-41fd-abd5-6d06817eebd5",
   "metadata": {},
   "source": [
    "<h3>Training using Age-based Scheduling</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72cf797-2b05-414b-95a6-a8c289c3c37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ABS_training(model_type, global_model, lr, total_clients, clients_per_round, total_rounds, local_epochs, training_loaders, validation_loader):\n",
    "    clients = [model_type().to(device) for _ in range(total_clients)]\n",
    "    \n",
    "    for models in clients:\n",
    "        models.load_state_dict(global_model.state_dict())\n",
    "    \n",
    "    opt = [optim.SGD(models.parameters(), lr=lr) for models in clients]\n",
    "    clients_age = np.zeros(total_clients)\n",
    "    \n",
    "    average_losses, valid_losses, valid_accuracies = [], [], []\n",
    "\n",
    "    global_start_time = time.time()\n",
    "    for round in range(total_rounds):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Age-based scheduling: select the clients with the highest age\n",
    "        old_clients_idx = np.argsort(clients_age)[-clients_per_round:]\n",
    "\n",
    "        clients_age += 1\n",
    "        clients_age[old_clients_idx] = 0  # Reset the age of the selected clients\n",
    "\n",
    "        client_losses = 0      \n",
    "        selected_models = []\n",
    "        \n",
    "        for i in range(clients_per_round):\n",
    "            clients[old_clients_idx[i]].load_state_dict(global_model.state_dict())\n",
    "            client_losses += client_update(clients[old_clients_idx[i]], opt[old_clients_idx[i]], training_loaders[old_clients_idx[i]], local_epochs)\n",
    "            selected_models.append(clients[old_clients_idx[i]])\n",
    "        \n",
    "\n",
    "        global_aggregate(global_model, selected_models)\n",
    "    \n",
    "        avg_loss = client_losses / clients_per_round\n",
    "        valid_loss , valid_accuracy = model_evaluation(global_model, validation_loader)\n",
    "\n",
    "        average_losses.append(avg_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accuracies.append(valid_accuracy)\n",
    "    \n",
    "        end_time = time.time()\n",
    "        round_time = end_time - start_time\n",
    "    \n",
    "        if (round % 10) == 0:\n",
    "            rounds_end_time = time.time()\n",
    "            rounds_time = rounds_end_time - global_start_time\n",
    "            print('Round {:3d}, Time (secs) {:.2f}: Average loss {:.4f}, Validation Loss {:.4f}, Validation Accuracy {:.4f}'.format(round + 1, rounds_time, avg_loss, valid_loss, valid_accuracy))\n",
    "    return average_losses, valid_losses, valid_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b493a9ae",
   "metadata": {},
   "source": [
    "<h3>Training using Age of Update (AoU) OR Data Shapley value Scheduling</h3>\n",
    "\n",
    "<t>If a UE’s k AoU surpasses the threshold or its Data Shapley value exceeds the Shapley value of the current highest value in the list, or both conditions are met,\n",
    "the UE k is positioned at the beginning of the list; otherwise, it is placed at the end</t>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb41348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_client_map(client_models, training_loaders):\n",
    "    client_map = {}\n",
    "    for idx, client_model in enumerate(client_models):\n",
    "        client_id = f\"client_{idx + 1}\"\n",
    "        optimizer = optim.SGD(client_model.parameters(), lr=lr)\n",
    "        client_map[client_id] = {\n",
    "            'model': client_model,\n",
    "            'optimizer': optimizer,\n",
    "            'training_loader': training_loaders[idx],\n",
    "            'accuracies': [],\n",
    "            'losses': [],\n",
    "            'shapley_value': random.random(),\n",
    "            'age': 0\n",
    "        }\n",
    "    return client_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd955f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_performance(client_map, selected_clients, shapley_threshold):\n",
    "    for client_id in client_map.keys():\n",
    "        if client_id in selected_clients:\n",
    "            client_accuracies = client_map[client_id][\"accuracies\"]\n",
    "            mean_acc = sum(client_accuracies) / len(client_accuracies)\n",
    "            variance = sum(((x - mean_acc) ** 2) for x in client_accuracies) / len(client_accuracies)\n",
    "            if variance > shapley_threshold:\n",
    "                updated_value = client_map[client_id][\"shapley_value\"] + 1.0    \n",
    "            else:\n",
    "                updated_value = client_map[client_id][\"shapley_value\"]\n",
    "        else:\n",
    "            updated_value = client_map[client_id][\"shapley_value\"] \n",
    "\n",
    "        mean_performance_score = (sum(client_map[client_id][\"losses\"]) + updated_value) / (len(client_map[client_id][\"losses\"]) + 1)\n",
    "        client_map[client_id][\"shapley_value\"] = mean_performance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fa9df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AoU_Scheduler(client_map, AoU_threshold, clients_per_round):\n",
    "    selected_clients = []\n",
    "    client_position = []\n",
    "    highest_score = max(client_map.values(), key=lambda x: x['shapley_value'])['shapley_value']\n",
    "    \n",
    "    for client_id, client_info in client_map.items():\n",
    "        client_score = client_info['shapley_value']\n",
    "        client_age = client_info['age']\n",
    "        \n",
    "        if client_score >= highest_score or client_age > AoU_threshold:\n",
    "            client_position.insert(0, client_id)  # Prepend the client ID\n",
    "        else:\n",
    "            client_position.append(client_id)\n",
    "\n",
    "    for i in range(clients_per_round):\n",
    "        selected_clients.append(client_position[i])\n",
    "\n",
    "    return selected_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1610f452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test_client(client_map, selected_clients, local_epochs, validation_loader, global_model, performance_threshold):\n",
    "    for client_id in selected_clients:\n",
    "        client_map[client_id]['model'].load_state_dict(global_model.state_dict())\n",
    "        model = client_map[client_id]['model']\n",
    "        model.train()\n",
    "\n",
    "        optimizer = client_map[client_id]['optimizer']\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(local_epochs):\n",
    "            for inputs, labels in client_map[client_id]['training_loader']:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        for inputs, labels in validation_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        accuracy = correct / (len(validation_loader.dataset))\n",
    "\n",
    "        client_map[client_id]['accuracies'].append(accuracy)\n",
    "        client_map[client_id]['losses'].append(loss.item())\n",
    "    \n",
    "    update_performance(client_map, selected_clients, performance_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b862cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AoU_OR_DataShapley_training(model_type, global_model, total_clients, clients_per_round, total_rounds, local_epochs, training_loaders, validation_loader, age_threshold, shapley_threshold):\n",
    "    client_models = [model_type().to(device) for _ in range(total_clients)]\n",
    "    for models in client_models:\n",
    "        models.load_state_dict(global_model.state_dict())\n",
    "\n",
    "    client_map = create_client_map(client_models, training_loaders)\n",
    "\n",
    "    average_losses, valid_losses, valid_accuracies = [], [], []\n",
    "\n",
    "    global_start_time = time.time()\n",
    "    for round in range(total_rounds):\n",
    "        start_time = time.time()\n",
    "\n",
    "        selected_clients = AoU_Scheduler(client_map, age_threshold, clients_per_round)\n",
    "\n",
    "        for client_id in client_map:\n",
    "            client_map[client_id]['age'] = client_map[client_id]['age'] ** 2 + 1\n",
    "\n",
    "        # Reset the age of selected clients to 0\n",
    "        for client_id in selected_clients:\n",
    "            client_map[client_id]['age'] = 0\n",
    "        \n",
    "        train_and_test_client(client_map, selected_clients, local_epochs, validation_loader, global_model, shapley_threshold)        \n",
    "        update_performance(client_map, selected_clients, shapley_threshold)\n",
    "        global_aggregate(global_model, [client_map[client_id]['model'] for client_id in selected_clients])\n",
    "\n",
    "        client_losses = sum(client_map[client_id]['losses'][-1] for client_id in selected_clients)\n",
    "    \n",
    "        avg_loss = client_losses / len(selected_clients)\n",
    "        valid_loss , valid_accuracy = model_evaluation(global_model, validation_loader)\n",
    "\n",
    "        average_losses.append(avg_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accuracies.append(valid_accuracy)\n",
    "    \n",
    "        end_time = time.time()\n",
    "        round_time = end_time - start_time\n",
    "    \n",
    "        if (round % 10) == 0:\n",
    "            rounds_end_time = time.time()\n",
    "            rounds_time = rounds_end_time - global_start_time\n",
    "            print('Round {:3d}, Time (secs) {:.2f}: Average loss {:.4f}, Validation Loss {:.4f}, Validation Accuracy {:.4f}'.format(round + 1, rounds_time, avg_loss, valid_loss, valid_accuracy))\n",
    "\n",
    "    return average_losses, valid_losses, valid_accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3294dda9",
   "metadata": {},
   "source": [
    "<h3>Training using Version Age-based (VAoI)</h3>\n",
    "\n",
    "<t></t>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb557ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAoI_client_map(client_models, training_loaders):\n",
    "    client_map = {}\n",
    "    for idx, client_model in enumerate(client_models):\n",
    "        client_id = f\"client_{idx + 1}\"\n",
    "        optimizer = optim.SGD(client_model.parameters(), lr=lr)\n",
    "        client_map[client_id] = {\n",
    "            'model': client_model,\n",
    "            'optimizer': optimizer,\n",
    "            'training_loader': training_loaders[idx],\n",
    "            'accuracies': [],\n",
    "            'losses': [],\n",
    "            'version_age': 0\n",
    "        }\n",
    "    return client_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c088dcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_norm(client_model, global_model):\n",
    "    client_weights = client_model.state_dict()\n",
    "    global_weights = global_model.state_dict()\n",
    "    for key in client_weights.keys():\n",
    "        norm = torch.norm(client_weights[key] - global_weights[key], p=1)\n",
    "        norm = norm.item()\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f2e4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAoI_Scheduler(client_map, global_model, tau, h=lambda x: np.exp(x)):\n",
    "    num_clients = len(client_map)\n",
    "    client_version_ages = []\n",
    "\n",
    "    # Calculate the version age for each client\n",
    "    for client_id, client_info in client_map.items():\n",
    "        model = client_info['model']\n",
    "        distance = manhattan_norm(model, global_model)\n",
    "        if distance >= tau:\n",
    "            client_info['version_age'] += 1\n",
    "        client_version_ages.append(client_info['version_age'])\n",
    "\n",
    "    # Calculate the selection probabilities based on version age\n",
    "    selection_probs = [h(age) for age in client_version_ages]\n",
    "    selection_probs = [prob / sum(selection_probs) for prob in selection_probs]\n",
    "\n",
    "    # Select the clients based on the probabilities\n",
    "    selected_clients = np.random.choice(list(client_map.keys()), size=int(0.1 * num_clients), p=selection_probs, replace=False)\n",
    "\n",
    "    # Reset the version age for the selected clients\n",
    "    for client_id in selected_clients:\n",
    "        client_map[client_id]['version_age'] = 0\n",
    "\n",
    "    return selected_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e6fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VAoI_training(model_type, global_model, lr, total_clients, clients_per_round, total_rounds, local_epochs, training_loaders, validation_loader, tau):\n",
    "    client_models = [model_type().to(device) for _ in range(total_clients)]\n",
    "    for models in client_models:\n",
    "        models.load_state_dict(global_model.state_dict())\n",
    "\n",
    "    client_map = VAoI_client_map(client_models, training_loaders)\n",
    "    \n",
    "    opt = [optim.SGD(models.parameters(), lr=lr) for models in client_models]\n",
    "    \n",
    "    average_losses, valid_losses, valid_accuracies = [], [], []\n",
    "\n",
    "    global_start_time = time.time()\n",
    "    for round in range(total_rounds):\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Use the VAoI scheduler to select the clients\n",
    "        selected_clients = VAoI_Scheduler(client_map, global_model, tau, h=lambda x: np.exp(x))\n",
    "\n",
    "        client_losses = 0\n",
    "        selected_models = []\n",
    "        for client_id in selected_clients:\n",
    "            client_info = client_map[client_id]\n",
    "            client_model = client_info['model']\n",
    "            client_optimizer = client_info['optimizer']\n",
    "            client_training_loader = client_info['training_loader']\n",
    "\n",
    "            # Load the global model weights to the selected client\n",
    "            client_model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "            # Perform local training on the selected client\n",
    "            client_loss = client_update(client_model, client_optimizer, client_training_loader, local_epochs)\n",
    "            client_losses += client_loss\n",
    "            selected_models.append(client_model)\n",
    "\n",
    "        # Aggregate the selected client models to update the global model\n",
    "        global_aggregate(global_model, selected_models)\n",
    "\n",
    "        avg_loss = client_losses / len(selected_clients)\n",
    "        valid_loss, valid_accuracy = model_evaluation(global_model, validation_loader)\n",
    "\n",
    "        average_losses.append(avg_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accuracies.append(valid_accuracy)\n",
    "\n",
    "        end_time = time.time()\n",
    "        round_time = end_time - start_time\n",
    "\n",
    "        if (round % 10) == 0:\n",
    "            rounds_end_time = time.time()\n",
    "            rounds_time = rounds_end_time - global_start_time\n",
    "            print('Round {:3d}, Time (secs) {:.2f}: Average loss {:.4f}, Validation Loss {:.4f}, Validation Accuracy {:.4f}'.format(round + 1, rounds_time, avg_loss, valid_loss, valid_accuracy))\n",
    "\n",
    "    return average_losses, valid_losses, valid_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba53e445",
   "metadata": {},
   "source": [
    "<h2>Hyperparameters for Training Experience</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4871860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration: use CUDA if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Learning configuration\n",
    "lr = 0.015\n",
    "total_rounds = 100  # Total number of training rounds\n",
    "\n",
    "# Client configuration\n",
    "total_clients = 100  # Total number of clients\n",
    "clients_per_round = 10  # Number of clients selected per round\n",
    "\n",
    "# Local training configuration\n",
    "local_batchsize = 50  # Batch size for local training\n",
    "local_epochs = 5  # Number of epochs for local training\n",
    "\n",
    "shapley_threshold = 0.8\n",
    "age_threshold = 10\n",
    "\n",
    "tau = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500a05ed",
   "metadata": {},
   "source": [
    "<h2>IID Data Preparation for MNIST and CIFAR-10 Dataset</h2>\n",
    "<t>*The IID data is shuffled and then divided up across 100 clients each receiving 600 examples.*  \n",
    "\n",
    "*Exclusively use independent and identically distributed i.i.d. distributions for CIFAR-10 due to the absence of a natural\n",
    "data user partition*</t>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fd5e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "MNISTtransform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# MNIST IID Dataset\n",
    "MNIST_training_dataset = datasets.MNIST(filepath, train=True, download=True, transform=MNISTtransform)\n",
    "MNIST_training_datasplit = torch.utils.data.random_split(MNIST_training_dataset, [int(MNIST_training_dataset.data.shape[0] / total_clients) for _ in range(total_clients)])\n",
    "MNIST_iid_training = [torch.utils.data.DataLoader(x, batch_size=local_batchsize, shuffle=True) for x in MNIST_training_datasplit]\n",
    "\n",
    "MNIST_validation_dataset = datasets.MNIST(filepath, train=False, download = True, transform=MNISTtransform)\n",
    "MNIST_iid_validation = torch.utils.data.DataLoader(MNIST_validation_dataset, batch_size=local_batchsize, shuffle=True)\n",
    "\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "CIFARtransform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL Image to tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
    "])\n",
    "\n",
    "dataset = datasets.CIFAR10(filepath, train=True, download=True, transform=CIFARtransform)\n",
    "\n",
    "CIFAR10_dataset = torch.utils.data.random_split(dataset, [len(dataset) // total_clients for _ in range(total_clients)])\n",
    "CIFAR10_training = [torch.utils.data.DataLoader(x, batch_size=local_batchsize, shuffle=True) for x in CIFAR10_dataset]\n",
    "\n",
    "CIFAR10_validation = torch.utils.data.DataLoader(datasets.CIFAR10(filepath, train=False, transform=CIFARtransform), batch_size=local_batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8950a6f7-30e7-48a1-98ac-355225b54c3d",
   "metadata": {},
   "source": [
    "<h3>MLP Model Training with MNIST IID</h3>\n",
    "\n",
    "Training is done using *'Random'* Scheduling, *'Age-based'* Scheduling, and *'Age of Update OR Data Shapley'* Scheduling (See RNG_training, ABS_training, AoU_OR_DataShapley)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "1c46bef3-dd48-451f-8877-c5d44cd4b540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round   1, Time (secs) 6.79: Average loss 2.0328, Validation Loss 0.0410, Validation Accuracy 0.4623\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[180], line 16\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# print(\"=== Training: Model - MLP, Schedule - Random, Data Distribution - MNIST IID  ===\")\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# iid_MNIST_RNG_avg_losses, iid_MNIST_RNG_eval_losses, iid_MNIST_RNG_accuracies = RNG_training(MLP, MNIST_iid_RNG, lr, total_clients, clients_per_round, total_rounds, local_epochs, MNIST_iid_training, MNIST_iid_validation)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# print(\"\\n=== Training: Model - MLP, Schedule - Age of Update OR Data Shapley, Data Distribution - MNIST IID ===\")\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# iid_MNIST_AoU_OR_DataShapley_avg_losses, iid_MNIST_AoU_OR_DataShapley_eval_losses, iid_MNIST_AoU_OR_DataShapley_accuracies = AoU_OR_DataShapley_training(MLP, MNIST_iid_AoU_OR_DataShapley, total_clients, clients_per_round, total_rounds, local_epochs, MNIST_iid_training, MNIST_iid_validation, age_threshold, shapley_threshold)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Training: Model - MLP, Schedule - Version Age of Information, Data Distribution - MNIST IID ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m iid_MNIST_VAoI_avg_losses, iid_MNIST_VAoI_eval_losses, iid_MNIST_VAoI_accuracies \u001b[38;5;241m=\u001b[39m VAoI_training(MLP, MNIST_iid_VAoI, lr, total_clients, clients_per_round, total_rounds, local_epochs, MNIST_iid_training, MNIST_iid_validation, tau)\n",
      "Cell \u001b[1;32mIn[177], line 31\u001b[0m, in \u001b[0;36mVAoI_training\u001b[1;34m(model_type, global_model, lr, total_clients, clients_per_round, total_rounds, local_epochs, training_loaders, validation_loader, tau)\u001b[0m\n\u001b[0;32m     28\u001b[0m client_model\u001b[38;5;241m.\u001b[39mload_state_dict(global_model\u001b[38;5;241m.\u001b[39mstate_dict())\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Perform local training on the selected client\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m client_loss \u001b[38;5;241m=\u001b[39m client_update(client_model, client_optimizer, client_training_loader, local_epochs)\n\u001b[0;32m     32\u001b[0m client_losses \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m client_loss\n\u001b[0;32m     33\u001b[0m selected_models\u001b[38;5;241m.\u001b[39mappend(client_model)\n",
      "Cell \u001b[1;32mIn[166], line 4\u001b[0m, in \u001b[0;36mclient_update\u001b[1;34m(client, optimizer, training_loader, epochs)\u001b[0m\n\u001b[0;32m      2\u001b[0m client\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(training_loader):\n\u001b[0;32m      5\u001b[0m         inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\aidan_000\\anaconda3\\envs\\Pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\aidan_000\\anaconda3\\envs\\Pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\aidan_000\\anaconda3\\envs\\Pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\aidan_000\\anaconda3\\envs\\Pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[1;34m(self, indices)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\aidan_000\\anaconda3\\envs\\Pytorch_env\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[1;32mc:\\Users\\aidan_000\\anaconda3\\envs\\Pytorch_env\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\aidan_000\\anaconda3\\envs\\Pytorch_env\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\aidan_000\\anaconda3\\envs\\Pytorch_env\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\Users\\aidan_000\\anaconda3\\envs\\Pytorch_env\\Lib\\site-packages\\torchvision\\transforms\\functional.py:174\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    172\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# MNIST_iid_RNG = MLP().to(device)\n",
    "# MNIST_iid_ABS = MLP().to(device)\n",
    "# MNIST_iid_AoU_OR_DataShapley = MLP().to(device)\n",
    "MNIST_iid_VAoI = MLP().to(device)\n",
    "\n",
    "# print(\"=== Training: Model - MLP, Schedule - Random, Data Distribution - MNIST IID  ===\")\n",
    "# iid_MNIST_RNG_avg_losses, iid_MNIST_RNG_eval_losses, iid_MNIST_RNG_accuracies = RNG_training(MLP, MNIST_iid_RNG, lr, total_clients, clients_per_round, total_rounds, local_epochs, MNIST_iid_training, MNIST_iid_validation)\n",
    "\n",
    "# print(\"\\n=== Training: Model - MLP, Schedule - Age-Based, Data Distribution - MNIST IID ===\")\n",
    "# iid_MNIST_ABS_avg_losses, iid_MNIST_ABS_eval_losses, iid_MNIST_ABS_accuracies = ABS_training(MLP, MNIST_iid_ABS, lr, total_clients, clients_per_round, total_rounds, local_epochs, MNIST_iid_training, MNIST_iid_validation)\n",
    "\n",
    "# print(\"\\n=== Training: Model - MLP, Schedule - Age of Update OR Data Shapley, Data Distribution - MNIST IID ===\")\n",
    "# iid_MNIST_AoU_OR_DataShapley_avg_losses, iid_MNIST_AoU_OR_DataShapley_eval_losses, iid_MNIST_AoU_OR_DataShapley_accuracies = AoU_OR_DataShapley_training(MLP, MNIST_iid_AoU_OR_DataShapley, total_clients, clients_per_round, total_rounds, local_epochs, MNIST_iid_training, MNIST_iid_validation, age_threshold, shapley_threshold)\n",
    "\n",
    "print(\"\\n=== Training: Model - MLP, Schedule - Version Age of Information, Data Distribution - MNIST IID ===\")\n",
    "iid_MNIST_VAoI_avg_losses, iid_MNIST_VAoI_eval_losses, iid_MNIST_VAoI_accuracies = VAoI_training(MLP, MNIST_iid_VAoI, lr, total_clients, clients_per_round, total_rounds, local_epochs, MNIST_iid_training, MNIST_iid_validation, tau)\n",
    "\n",
    "# # Save Final Models\n",
    "# torch.save(MNIST_iid_RNG.state_dict(), '.\\\\Models\\\\MNIST_iid_RNG.pth')\n",
    "# torch.save(MNIST_iid_ABS.state_dict(), '.\\\\Models\\\\MNIST_iid_ABS.pth')\n",
    "# torch.save(MNIST_iid_AoU_OR_DataShapley.state_dict(), '.\\\\Models\\\\MNIST_iid_AoU_OR_DataShapley.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df303c0",
   "metadata": {},
   "source": [
    "<h4>MLP Model Training/Inferencing Experience Comparison for MNIST IID</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9392157",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'=================================== Final MLP Model Accuracies per Schedule ====================================')\n",
    "print(f'Random Scheduled MNIST IID Model Accuracy: {iid_MNIST_RNG_accuracies[-1]},\\nAge-based Scheduled MNIST IID  Model Accuracy: {iid_MNIST_ABS_accuracies[-1]},\\nAoU OR DataShapley Scheduled MNIST IID Model Accuracy: {iid_MNIST_AoU_OR_DataShapley_accuracies[-1]}')\n",
    "print(f'================================================================================================================')\n",
    "\n",
    "epochs_range = range(1, total_rounds + 1)\n",
    "\n",
    "# Plot Global Training Loss\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, iid_MNIST_RNG_eval_losses, color='red', linestyle=\"dashed\", label=\"Random Schedule\")\n",
    "plt.plot(epochs_range, iid_MNIST_ABS_eval_losses, color='blue', linestyle=\"dotted\", label=\"Age-based Schedule\")\n",
    "plt.plot(epochs_range, iid_MNIST_AoU_OR_DataShapley_eval_losses, color='green', linestyle=\"solid\", label=\"AoU OR DataShapley Schedule\")\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.legend(loc='upper right')  \n",
    "plt.title('MLP MNIST IID Loss Curve')\n",
    "\n",
    "# Plot Global Validation Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, iid_MNIST_RNG_accuracies, color='red', linestyle=\"dashed\", label=\"Random Schedule\")\n",
    "plt.plot(epochs_range, iid_MNIST_ABS_accuracies, color='blue', linestyle=\"dotted\", label=\"Age-based Schedule\")\n",
    "plt.plot(epochs_range, iid_MNIST_AoU_OR_DataShapley_accuracies, color='green', linestyle=\"solid\", label=\"AoU OR DataShapley Schedule\")\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.legend(loc='lower right') \n",
    "plt.title('MLP MNIST IID Accuracy Curve')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('.\\\\Plots\\\\mnist_iid_results.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebeb150",
   "metadata": {},
   "source": [
    "<h3>CNN Model Training with CIFAR-10 IID</h3>\n",
    "\n",
    "Training is done using *'Random'* Scheduling, *'Age-based'* Scheduling, and *'Age of Update OR Data Shapley'* Scheduling (See RNG_training, ABS_training, AoU_OR_DataShapley)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695659a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR10_iid_RNG = CNN().to(device)\n",
    "# CIFAR10_iid_ABS = CNN().to(device)\n",
    "# CIFAR10_iid_AoU_OR_DataShapley = CNN().to(device)\n",
    "\n",
    "# print(\"=== Training: Model - CNN, Schedule - Random, Data Distribution - IID CIFAR-10 ===\")\n",
    "# iid_CIFAR10_RNG_avg_losses, iid_CIFAR10_RNG_eval_losses, iid_CIFAR10_RNG_accuracies = RNG_training(CNN, CIFAR10_iid_RNG, lr, total_clients, clients_per_round, total_rounds, local_epochs, CIFAR10_training, CIFAR10_validation)\n",
    "\n",
    "# print(\"\\n=== Training: Model - CNN, Schedule - Age-Based, Data Distribution - IID CIFAR-10 ===\")\n",
    "# iid_CIFAR10_ABS_avg_losses, iid_CIFAR10_ABS_eval_losses, iid_CIFAR10_ABS_accuracies = ABS_training(CNN, CIFAR10_iid_ABS, lr, total_clients, clients_per_round, total_rounds, local_epochs, CIFAR10_training, CIFAR10_validation)\n",
    "\n",
    "# print(\"\\n=== Training: Model - CNN, Schedule - Age of Update OR Data Shapley, Data Distribution - IID CIFAR-10 ===\")\n",
    "# iid_CIFAR10_AoU_OR_DataShapley_avg_losses, iid_CIFAR10_AoU_OR_DataShapley_eval_losses, iid_CIFAR10_AoU_OR_DataShapley_accuracies = AoU_OR_DataShapley_training(CNN, CIFAR10_iid_AoU_OR_DataShapley, lr, total_clients, clients_per_round, total_rounds, local_epochs, CIFAR10_training, CIFAR10_validation, age_threshold, shapley_threshold)\n",
    "\n",
    "# # Save Final Models\n",
    "# torch.save(CIFAR10_iid_RNG.state_dict(), '.\\\\Models\\\\CIFAR10_iid_RNG.pth')\n",
    "# torch.save(CIFAR10_iid_ABS.state_dict(), '.\\\\Models\\\\CIFAR10_iid_ABS.pth')\n",
    "# torch.save(CIFAR10_iid_AoU_OR_DataShapley.state_dict(), '.\\\\Models\\\\CIFAR10_iid_AoU_OR_DataShapley.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcbe39c",
   "metadata": {},
   "source": [
    "<h4>CNN Model Training/Inferencing Experience Comparison for CIFAR-10 IID</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff00339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'=================================== Final CNN Model Accuracies per Schedule ====================================')\n",
    "print(f'Random Scheduled CIFAR-10 IID Model Accuracy: {iid_CIFAR10_RNG_accuracies[-1]},\\nAge-based Scheduled CIFAR-10 IID  Model Accuracy: {iid_CIFAR10_ABS_accuracies[-1]},\\nAoU OR DataShapley Scheduled CIFAR-10 IID Model Accuracy: {iid_CIFAR10_AoU_OR_DataShapley_accuracies[-1]}')\n",
    "print(f'================================================================================================================')\n",
    "\n",
    "epochs_range = range(1, total_rounds + 1)\n",
    "\n",
    "# Plot Global Training Loss\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, iid_CIFAR10_RNG_eval_losses, color='red', linestyle=\"dashed\", label=\"Random Schedule\")\n",
    "plt.plot(epochs_range, iid_CIFAR10_ABS_eval_losses, color='blue', linestyle=\"dotted\", label=\"Age-based Schedule\")\n",
    "plt.plot(epochs_range, iid_CIFAR10_AoU_OR_DataShapley_eval_losses, color='green', linestyle=\"solid\", label=\"AoU OR DataShapley Schedule\")\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.legend(loc='upper right')  \n",
    "plt.title('CNN CIFAR-10 IID Loss Curve')\n",
    "\n",
    "# Plot Global Validation Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, iid_CIFAR10_RNG_accuracies, color='red', linestyle=\"dashed\", label=\"Random Schedule\")\n",
    "plt.plot(epochs_range, iid_CIFAR10_ABS_accuracies, color='blue', linestyle=\"dotted\", label=\"Age-based Schedule\")\n",
    "plt.plot(epochs_range, iid_CIFAR10_AoU_OR_DataShapley_accuracies, color='green', linestyle=\"solid\", label=\"AoU OR DataShapley Schedule\")\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.legend(loc='lower right') \n",
    "plt.title('CNN CIFAR-10 IID Accuracy Curve')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('.\\\\Plots\\\\cifar10_iid_results.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abe98ad",
   "metadata": {},
   "source": [
    "<h2>Non-IID Data Preparation for MNIST Dataset</h2>\n",
    "\n",
    "<t>*The Non-IID data is sorted by digit label, divided up into 200 'shards' of 300 examples, and then each client receieves 2 'shards'*</t>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e5d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST Non-IID Dataset\n",
    "training_labels = torch.stack([MNIST_training_dataset.targets == i for i in range(10)])\n",
    "training_labels_split = []\n",
    "\n",
    "for i in range(5):\n",
    "    training_labels_split += torch.split(torch.where(training_labels[(2 * i):(2 * (i + 1))].sum(0))[0], int(len(MNIST_training_dataset.data) / total_clients))\n",
    "training_dataset_split = [torch.utils.data.Subset(MNIST_training_dataset, labels) for labels in training_labels_split]\n",
    "MNIST_noniid_training = [torch.utils.data.DataLoader(x, batch_size=local_batchsize, shuffle=True) for x in training_dataset_split]\n",
    "\n",
    "MNIST_noniid_validation = torch.utils.data.DataLoader(MNIST_validation_dataset, batch_size=local_batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615ae80f-38f6-483c-a801-e96c63f24e1c",
   "metadata": {},
   "source": [
    "<h3>MLP Model Training with MNIST Non-IID</h3>\n",
    "\n",
    "Training is done using *'Random'* Scheduling, *'Age-based'* Scheduling, and *'Age of Update OR Data Shapley'* Scheduling (See RNG_training, ABS_training, AoU_OR_DataShapley)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42f6dfe-7a37-474a-bf51-3f45ffd92be6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MNIST_noniid_RNG = MLP().to(device)\n",
    "# MNIST_noniid_ABS = MLP().to(device)\n",
    "# MNIST_noniid_AoU_OR_DataShapley = MLP().to(device)\n",
    "\n",
    "# print(\"=== Training: Model - MLP, Schedule - Random, Data Distribution - MNIST Non-IID  ===\")\n",
    "# noniid_MNIST_RNG_avg_losses, noniid_MNIST_RNG_eval_losses, noniid_MNIST_RNG_accuracies = RNG_training(MLP, MNIST_noniid_RNG, lr, total_clients, clients_per_round, total_rounds, local_epochs, MNIST_noniid_training, MNIST_noniid_validation)\n",
    "\n",
    "# print(\"\\n=== Training: Model - MLP, Schedule - Age-Based, Data Distribution - MNIST Non-IID  ===\")\n",
    "# noniid_MNIST_ABS_avg_losses, noniid_MNIST_ABS_eval_losses, noniid_MNIST_ABS_accuracies = ABS_training(MLP, MNIST_noniid_ABS, lr, total_clients, clients_per_round, total_rounds, local_epochs, MNIST_noniid_training, MNIST_noniid_validation)\n",
    "\n",
    "# print(\"\\n=== Training: Model - MLP, Schedule - Age of Update OR Data Shapley, Data Distribution - MNIST Non-IID  ===\")\n",
    "# noniid_MNIST_AoU_OR_DataShapley_avg_losses, noniid_MNIST_AoU_OR_DataShapley_eval_losses, noniid_MNIST_AoU_OR_DataShapley_accuracies = AoU_OR_DataShapley_training(MLP, MNIST_noniid_AoU_OR_DataShapley, lr, total_clients, clients_per_round, total_rounds, local_epochs, MNIST_noniid_training, MNIST_noniid_validation, age_threshold, shapley_threshold)\n",
    "\n",
    "# torch.save(MNIST_noniid_RNG.state_dict(), '.\\\\Models\\\\MNIST_noniid_RNG.pth')\n",
    "# torch.save(MNIST_noniid_ABS.state_dict(), '.\\\\Models\\\\MNIST_noniid_ABS.pth')\n",
    "# torch.save(MNIST_noniid_AoU_OR_DataShapley.state_dict(), '.\\\\Models\\\\MNIST_noniid_AoU_OR_DataShapley.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a174e20d",
   "metadata": {},
   "source": [
    "<h4>MLP Model Training/Inferencing Experience Comparison for MNIST Non-IID</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54d30bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'=================================== Final MLP Model Accuracies per Schedule ====================================')\n",
    "print(f'Random Scheduled MNIST Non-IID Model Accuracy: {noniid_MNIST_RNG_accuracies[-1]},\\nAge-based Scheduled MNIST Non-IID  Model Accuracy: {noniid_MNIST_ABS_accuracies[-1]},\\nAoU OR DataShapley Scheduled MNIST Non-IID Model Accuracy: {noniid_MNIST_AoU_OR_DataShapley_accuracies[-1]}')\n",
    "print(f'================================================================================================================')\n",
    "\n",
    "epochs_range = range(1, total_rounds + 1)\n",
    "\n",
    "# Plot Global Training Loss\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, noniid_MNIST_RNG_eval_losses, color='red', linestyle=\"dashed\", label=\"Random Schedule\")\n",
    "plt.plot(epochs_range, noniid_MNIST_ABS_eval_losses, color='blue', linestyle=\"dotted\", label=\"Age-based Schedule\")\n",
    "plt.plot(epochs_range, noniid_MNIST_AoU_OR_DataShapley_eval_losses, color='green', linestyle=\"solid\", label=\"AoU OR DataShapley Schedule\")\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.legend(loc='upper right')  \n",
    "plt.title('MLP MNIST Non-IID Loss Curve')\n",
    "\n",
    "# Plot Global Validation Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, noniid_MNIST_RNG_accuracies, color='red', linestyle=\"dashed\", label=\"Random Schedule\")\n",
    "plt.plot(epochs_range, noniid_MNIST_ABS_accuracies, color='blue', linestyle=\"dotted\", label=\"Age-based Schedule\")\n",
    "plt.plot(epochs_range, noniid_MNIST_AoU_OR_DataShapley_accuracies, color='green', linestyle=\"solid\", label=\"AoU OR DataShapley Schedule\")\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.legend(loc='lower right') \n",
    "plt.title('MLP MNIST Non-IID Accuracy Curve')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('.\\\\Plots\\\\mnist_noniid_results.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a5be7-0d5f-4a5f-bb66-3f6d90f0f40f",
   "metadata": {},
   "source": [
    "<h2>Global Model Complexity Comparisons</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c8424-d84b-46d3-9113-df79df2d5e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_summary(model, model_name, dataset_name):\n",
    "    if dataset_name == \"MNIST\":\n",
    "        input_size = (1, 28, 28)\n",
    "    elif dataset_name == \"CIFAR-10\":\n",
    "        input_size = (3, 32, 32)\n",
    "\n",
    "    model_stats = summary(model, input_size=input_size, col_width=16, col_names=[\"kernel_size\", \"output_size\", \"num_params\", \"mult_adds\"], row_settings=[\"var_names\"])\n",
    "    print(f\"\\n{'='*30} {model_name} {'='*30}\")\n",
    "    print(model_stats)\n",
    "    print('='*80)\n",
    "\n",
    "print(\"\\t\\t\\t\\tMLP Model Comparisons using MNIST IID\\n\")\n",
    "print_model_summary(MNIST_iid_RNG, \"Random IID Model\", \"MNIST\")\n",
    "print_model_summary(MNIST_iid_ABS, \"Age-Based IID Model\", \"MNIST\")\n",
    "print_model_summary(MNIST_iid_AoU_OR_DataShapley, \"AoU OR DataShapley IID Model\", \"MNIST\")\n",
    "print()\n",
    "\n",
    "print(\"\\t\\t\\t\\tMLP Model Comparisons using MNIST Non-IID\\n\")\n",
    "print_model_summary(MNIST_noniid_RNG, \"Random Non-IID Model\", \"MNIST\")\n",
    "print_model_summary(MNIST_noniid_ABS, \"Age-Based Non-IID Model\", \"MNIST\")\n",
    "print_model_summary(MNIST_noniid_AoU_OR_DataShapley, \"AoU OR DataShapley Non-IID Model\", \"MNIST\")\n",
    "print()\n",
    "\n",
    "print(\"\\t\\t\\t\\tCNN Model Comparisons using CIFAR10 IID\\n\")\n",
    "print_model_summary(CIFAR10_iid_RNG, \"Random IID Model\", \"CIFAR-10\")\n",
    "print_model_summary(CIFAR10_iid_ABS, \"Age-Based IID Model\", \"CIFAR-10\")\n",
    "print_model_summary(CIFAR10_iid_AoU_OR_DataShapley, \"AoU OR DataShapley IID Model\", \"CIFAR-10\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
