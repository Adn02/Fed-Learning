{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Global Dependencies\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Import Helper Libaries\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "filepath = \"C:/Users/aidan_000/Desktop/UNCC/Github/Fed-Learning/data\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>CIFAR-10 CNN Model Architecture</h2>\n",
    "\n",
    "<t>*The second experiment focuses on training a Convolutional Neural Network CNN on the CIFAR-10 dataset. The CNN\n",
    "architecture encompasses two convolutional layers with max pooling, two fully connected layers, and a softmax output\n",
    "layer*</t>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n",
    "        self.fc1 = nn.Linear(64*8*8, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Federated Learning Algorithms</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Global Aggregator with Selected Clients' quantized</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_aggregate(global_model, client_models):\n",
    "    global_dict = global_model.state_dict()\n",
    "    quantized_updates = []\n",
    "    for i, client_model in enumerate(client_models):\n",
    "        # Quantize the difference between the client's final local model and the global model\n",
    "        quantized_update = Q(client_model.state_dict() - global_dict)\n",
    "        quantized_updates.append(quantized_update)\n",
    "    \n",
    "    # Aggregate the quantized updates and update the global model\n",
    "    for k in global_dict.keys():\n",
    "        global_dict[k] = global_dict[k] + (1 / len(client_models)) * torch.stack(\n",
    "            [update[k] for update in quantized_updates], 0\n",
    "        ).mean(0)\n",
    "    \n",
    "    global_model.load_state_dict(global_dict)\n",
    "    \n",
    "    # Update the client models with the new global model\n",
    "    for model in client_models:\n",
    "        model.load_state_dict(global_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Global Model Evaluation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(global_model, validation_loader):\n",
    "    global_model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validation_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            output = global_model(inputs)\n",
    "            loss += nn.CrossEntropyLoss()(output, labels).item()  # Using the criterion\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    loss /= len(validation_loader.dataset)\n",
    "    accuracy = correct / len(validation_loader.dataset)\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Client Update</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_update(client, optimizer, training_loader, epochs):\n",
    "    client.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (inputs, labels) in enumerate(training_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = client(inputs)\n",
    "            loss = F.cross_entropy(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Training using Random Scheduling</h3>\n",
    "\n",
    "1. Periodic averaging: Let the participating nodes conduct a number of local updates and synchronize through the parameter server periodically. To be more specific, once nodes pull an updated model from the server, they update the model locally by running iterations of the SGD method and then send proper information to the server for updating the aggregate model.\n",
    "\n",
    "2. Partial node participation: Each round of the training algorithm the parameter server sends its current model X<sub>k</sub> to all the _r_ nodes in subset _S<sub>k</sub>_, which are distributed uniformly at random among the total _n_ nodes\n",
    "\n",
    "3. Quantized message-passing: each node _i_ ∈ _S<sub>k</sub>_ obtains the model **x**<sup>(_i_)</sup><sub>_k,τ_</sub> after running τ local iterations of an optimization method (SGD) on the most recent model **x**<sub>_k_</sub> that it has receieved from the server. Then each node _i_ applies a quantizer operator _Q(·)_ on the difference between the recieved model and its updated model, i.e., **x**<sup>(_i_)</sup><sub>_k,τ_</sub>  -  **x**<sub>_k_</sub> and then uploads the quantized vector _Q(**x**<sup>(i)</sup><sub>k,τ</sub>  -  **x**<sub>k</sub>)_ to the parameter server. Once these quantized vectors are sent to the server, it decodes the quantized signals and combines them to come up with a new model **x**<sub>_k_+1</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNG_training(model_type, global_model, lr, total_clients, clients_per_round, total_rounds, local_epochs, training_loaders, validation_loader):\n",
    "    clients = [model_type().to(device) for _ in range(total_clients)]\n",
    "\n",
    "    for models in clients:\n",
    "        models.load_state_dict(global_model.state_dict())\n",
    "    \n",
    "    opt = [optim.SGD(models.parameters(), lr=lr) for models in clients]\n",
    "\n",
    "    average_losses, valid_losses, valid_accuracies = [], [], []\n",
    "\n",
    "    global_start_time = time.time()\n",
    "    for round in range(total_rounds):\n",
    "        start_time = time.time()\n",
    "        clients_idx = np.random.permutation(total_clients)[:clients_per_round]\n",
    "\n",
    "        client_losses = 0\n",
    "        selected_models = []\n",
    "        \n",
    "        for i in range(clients_per_round):\n",
    "            clients[clients_idx[i]].load_state_dict(global_model.state_dict())\n",
    "            client_losses += client_update(clients[clients_idx[i]], opt[clients_idx[i]], training_loaders[clients_idx[i]], local_epochs)\n",
    "            selected_models.append(clients[clients_idx[i]])\n",
    "        \n",
    "        global_aggregate(global_model, selected_models)\n",
    "    \n",
    "        avg_loss = client_losses / clients_per_round\n",
    "        valid_loss , valid_accuracy = model_evaluation(global_model, validation_loader)\n",
    "\n",
    "        average_losses.append(avg_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        valid_accuracies.append(valid_accuracy)\n",
    "    \n",
    "        end_time = time.time()\n",
    "        round_time = end_time - start_time\n",
    "    \n",
    "        if (round % 10) == 0:\n",
    "            rounds_end_time = time.time()\n",
    "            rounds_time = rounds_end_time - global_start_time\n",
    "            print('Round {:3d}, Time (secs) {:.2f}: Average loss {:.4f}, Validation Loss {:.4f}, Validation Accuracy {:.4f}'.format(round + 1, rounds_time, avg_loss, valid_loss, valid_accuracy))\n",
    "    return average_losses, valid_losses, valid_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Hyperparameters for Training Experience</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration: use CUDA if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Learning configuration\n",
    "lr = 0.015\n",
    "total_rounds = 100  # Total number of training rounds, Denoted as T\n",
    "\n",
    "# Client configuration\n",
    "total_clients = 100  # Total number of clients\n",
    "clients_per_round = 0.2*total_clients  # Number of clients selected per round, Denoted as r\n",
    "\n",
    "# Local training configuration\n",
    "local_batchsize = 10  # Batch size for local training\n",
    "local_epochs = 10  # Denoted as tau (little T)\n",
    "\n",
    "quantization_level = 1 # Denoted as s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>IID Data Preparation for the CIFAR-10 Dataset</h2>\n",
    "<t>*The IID data is shuffled and then divided up across 100 clients each receiving 600 examples.*  \n",
    "\n",
    "*Exclusively use independent and identically distributed i.i.d. distributions for CIFAR-10 due to the absence of a natural\n",
    "data user partition*</t>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "CIFARtransform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert PIL Image to tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n",
    "])\n",
    "\n",
    "dataset = datasets.CIFAR10(filepath, train=True, download=True, transform=CIFARtransform)\n",
    "\n",
    "CIFAR10_dataset = torch.utils.data.random_split(dataset, [len(dataset) // total_clients for _ in range(total_clients)])\n",
    "CIFAR10_training = [torch.utils.data.DataLoader(x, batch_size=local_batchsize, shuffle=True) for x in CIFAR10_dataset]\n",
    "\n",
    "CIFAR10_validation = torch.utils.data.DataLoader(datasets.CIFAR10(filepath, train=False, transform=CIFARtransform), batch_size=local_batchsize, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>CNN Model Training with CIFAR-10 IID</h2>\n",
    "\n",
    "<t>Training is done using *'Random'* Scheduling</t>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_FedPAQ = CNN().to(device)\n",
    "# CNN_FedAvg = CNN().to(device)\n",
    "\n",
    "print(\"=== Training: Model - FedPAQ, Schedule - Random, Data Distribution - IID CIFAR-10 ===\")\n",
    "FedPAQ_losses, FedPAQ_eval_losses, FedPAQ_accuracies = RNG_training(CNN, CNN_FedPAQ, lr, total_clients, clients_per_round, total_rounds, local_epochs, CIFAR10_training, CIFAR10_validation)\n",
    "\n",
    "# Save Final Models\n",
    "torch.save(CIFAR10_iid_RNG.state_dict(), '.\\\\Models\\\\CNN_FedPAQ.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>CNN Model Training/Inferencing Experience Comparison for CIFAR-10 IID</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'=================================== Final CNN Model Accuracies per Schedule ====================================')\n",
    "print(f'Random Scheduled FedPAQ CIFAR10 IID Model Accuracy: {FedPAQ_accuracies[-1]}')\n",
    "print(f'================================================================================================================')\n",
    "\n",
    "epochs_range = range(1, total_rounds + 1)\n",
    "\n",
    "# Plot Global Training Loss\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, FedPAQ_eval_losses, color='red', linestyle=\"dashed\", label=\"Random Schedule\")\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.legend(loc='upper right')  \n",
    "plt.title('CNN CIFAR-10 IID Loss Curve')\n",
    "\n",
    "# Plot Global Validation Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, FedPAQ_accuracies, color='red', linestyle=\"dashed\", label=\"Random Schedule\")\n",
    "plt.xlabel('Communication Rounds')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.legend(loc='lower right') \n",
    "plt.title('CNN CIFAR-10 IID Accuracy Curve')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('.\\\\Plots\\\\cifar10_iid_FedPAQ_results.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
